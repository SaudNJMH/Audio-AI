{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d8baa1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "This section imports required libraries and the pyannote pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43345fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.n.h/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pyannote.audio import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13bafb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific UserWarnings from rich.live about ipywidgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message='install \"ipywidgets\" for Jupyter support')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0802a",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "This section loads the refined dataset CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebd582b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...</td>\n",
       "      <td>[{\"start\":0.025320884681576335,\"end\":11.079020...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...</td>\n",
       "      <td>[{\"start\":0.1782457853618421,\"end\":116.0241887...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...   \n",
       "1  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...   \n",
       "\n",
       "                                             speaker  speaker_count  \n",
       "0  [{\"start\":0.025320884681576335,\"end\":11.079020...            1.0  \n",
       "1  [{\"start\":0.1782457853618421,\"end\":116.0241887...            1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/refined_dataset.csv\")\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7948f77",
   "metadata": {},
   "source": [
    "## 3. Parse and Inspect Segments\n",
    "This section parses the speaker segments and inspects the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16255449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: ../audios-wav/12-audios-ar-en/6-audios-ar/1_speaker_ar/solo10_ar.wav\n",
      "Speaker count (ground truth): 1.0\n",
      "First 2 segments: [{'start': 0.025320884681576335, 'end': 11.079020083885514, 'channel': 0, 'labels': ['Speaker 1']}, {'start': 11.353187839068042, 'end': 28.60528196857287, 'channel': 0, 'labels': ['Speaker 1']}]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Parse the JSON-like string into Python objects\n",
    "df[\"segments\"] = df[\"speaker\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "# Quick check on the first row\n",
    "print(\"Audio file:\", df.loc[0, \"audio\"])\n",
    "print(\"Speaker count (ground truth):\", df.loc[0, \"speaker_count\"])\n",
    "print(\"First 2 segments:\", df.loc[0, \"segments\"][:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf2d25",
   "metadata": {},
   "source": [
    "## 4. Load and Run PyAnnote Pipeline\n",
    "This section loads the pyannote pipeline, runs diarization, and collects predictions for a sample audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82543bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[INFO] Loading pyannote pipeline...\n",
      "[INFO] Pipeline loaded in 2.73 sec\n",
      "[INFO] Starting diarization for: ../audios-wav/12-audios-ar-en/6-audios-ar/1_speaker_ar/solo10_ar.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/s.n.h/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/pyannote/audio/models/blocks/pooling.py:104: \n",
       "UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor (input\n",
       "numel divided by output numel). (Triggered internally at \n",
       "/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/s.n.h/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/pyannote/audio/models/blocks/pooling.py:104: \n",
       "UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input\n",
       "numel divided by output numel). (Triggered internally at \n",
       "/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished diarization in 243.46 sec\n",
      "[INFO] Total segments detected: 7\n",
      "Preview (first 5): [\n",
      "  {\n",
      "    \"start\": 0.03096875,\n",
      "    \"end\": 208.97721875000002,\n",
      "    \"labels\": [\n",
      "      \"SPEAKER_00\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"start\": 17.918468750000002,\n",
      "    \"end\": 18.812843750000003,\n",
      "    \"labels\": [\n",
      "      \"SPEAKER_01\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"start\": 71.12534375,\n",
      "    \"end\": 71.95221875,\n",
      "    \"labels\": [\n",
      "      \"SPEAKER_01\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"start\": 83.14034375,\n",
      "    \"end\": 83.32596875,\n",
      "    \"labels\": [\n",
      "      \"SPEAKER_01\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"start\": 115.13534375,\n",
      "    \"end\": 115.37159375,\n",
      "    \"labels\": [\n",
      "      \"SPEAKER_01\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "os.environ[\"SPEECHBRAIN_LOCAL_STRATEGY\"] = \"copy\"\n",
    "# Load token from .env\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "assert hf_token is not None, \"HUGGINGFACE_TOKEN not found in .env\"\n",
    "\n",
    "# Load diarization pipeline\n",
    "print(\"[INFO] Loading pyannote pipeline...\")\n",
    "start_time = time.time()\n",
    "# make progress bar for pyannote with hook()\n",
    "\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=hf_token)\n",
    "print(f\"[INFO] Pipeline loaded in {time.time() - start_time:.2f} sec\")\n",
    "pipeline.to(device)\n",
    "# Pick one audio for now\n",
    "test_audio = df.loc[0, \"audio\"]\n",
    "print(f\"[INFO] Starting diarization for: {test_audio}\")\n",
    "\n",
    "file_start = time.time()\n",
    "with ProgressHook() as hook:\n",
    "    diarization = pipeline(test_audio, hook=hook)\n",
    "    file_end = time.time()\n",
    "\n",
    "# Collect predictions\n",
    "pred_segments = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    pred_segments.append({\n",
    "        \"start\": float(turn.start),\n",
    "        \"end\": float(turn.end),\n",
    "        \"labels\": [speaker]\n",
    "    })\n",
    "\n",
    "print(f\"[INFO] Finished diarization in {file_end - file_start:.2f} sec\")\n",
    "print(f\"[INFO] Total segments detected: {len(pred_segments)}\")\n",
    "print(\"Preview (first 5):\", json.dumps(pred_segments[:5], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895267b",
   "metadata": {},
   "source": [
    "## 5. Batch Diarization and Save Results\n",
    "This section runs diarization on all files and saves the results to JSON and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0ddaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] (1/12) Processing solo10_ar ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished solo10_ar in 242.92 sec (7 segments)\n",
      "\n",
      "[INFO] (2/12) Processing solo3_ar ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished solo3_ar in 219.94 sec (62 segments)\n",
      "\n",
      "[INFO] (3/12) Processing two_speakers7_ar ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished two_speakers7_ar in 215.25 sec (37 segments)\n",
      "\n",
      "[INFO] (4/12) Processing two_speakers10_ar ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished two_speakers10_ar in 282.31 sec (80 segments)\n",
      "\n",
      "[INFO] (5/12) Processing three_speakers5_ar ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished three_speakers5_ar in 362.47 sec (107 segments)\n",
      "\n",
      "[INFO] (6/12) Processing three_speakers1_ar ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished three_speakers1_ar in 296.23 sec (42 segments)\n",
      "\n",
      "[INFO] (7/12) Processing solo3_en ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished solo3_en in 205.73 sec (11 segments)\n",
      "\n",
      "[INFO] (8/12) Processing solo2_en ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished solo2_en in 202.76 sec (43 segments)\n",
      "\n",
      "[INFO] (9/12) Processing two_speakers8_en ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished two_speakers8_en in 301.70 sec (85 segments)\n",
      "\n",
      "[INFO] (10/12) Processing two_speakers7_en ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished two_speakers7_en in 303.20 sec (35 segments)\n",
      "\n",
      "[INFO] (11/12) Processing three_speakers2_en ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished three_speakers2_en in 280.54 sec (111 segments)\n",
      "\n",
      "[INFO] (12/12) Processing three_speakers8_en ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished three_speakers8_en in 395.84 sec (68 segments)\n",
      "\n",
      "[INFO] All files processed. Summary saved to ../results/pyannote_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>n_segments</th>\n",
       "      <th>runtime_sec</th>\n",
       "      <th>output_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...</td>\n",
       "      <td>7</td>\n",
       "      <td>242.921457</td>\n",
       "      <td>../results/pyannote_predictions/solo10_ar_pyan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...</td>\n",
       "      <td>62</td>\n",
       "      <td>219.935136</td>\n",
       "      <td>../results/pyannote_predictions/solo3_ar_pyann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...</td>\n",
       "      <td>37</td>\n",
       "      <td>215.254184</td>\n",
       "      <td>../results/pyannote_predictions/two_speakers7_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...</td>\n",
       "      <td>80</td>\n",
       "      <td>282.313628</td>\n",
       "      <td>../results/pyannote_predictions/two_speakers10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...</td>\n",
       "      <td>107</td>\n",
       "      <td>362.471811</td>\n",
       "      <td>../results/pyannote_predictions/three_speakers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  n_segments  runtime_sec  \\\n",
       "0  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...           7   242.921457   \n",
       "1  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...          62   219.935136   \n",
       "2  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...          37   215.254184   \n",
       "3  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...          80   282.313628   \n",
       "4  ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...         107   362.471811   \n",
       "\n",
       "                                         output_file  \n",
       "0  ../results/pyannote_predictions/solo10_ar_pyan...  \n",
       "1  ../results/pyannote_predictions/solo3_ar_pyann...  \n",
       "2  ../results/pyannote_predictions/two_speakers7_...  \n",
       "3  ../results/pyannote_predictions/two_speakers10...  \n",
       "4  ../results/pyannote_predictions/three_speakers...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "results = []\n",
    "output_dir = \"../results/pyannote_predictions\"\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    audio_path = row[\"audio\"]\n",
    "    audio_name = pathlib.Path(audio_path).stem\n",
    "    print(f\"\\n[INFO] ({idx+1}/{len(df)}) Processing {audio_name} ...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        with ProgressHook() as hook:\n",
    "            diarization = pipeline(audio_path, hook=hook)\n",
    "\n",
    "        pred_segments = []\n",
    "        # Map unique speakers to Speaker 1, Speaker 2, ...\n",
    "        speaker_map = {s: f\"Speaker {i+1}\" for i, s in enumerate(sorted(set(label for _, _, label in diarization.itertracks(yield_label=True))))}\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            pred_segments.append({\n",
    "                \"start\": float(turn.start),\n",
    "                \"end\": float(turn.end),\n",
    "                \"labels\": [speaker_map[speaker]]\n",
    "            })\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"[INFO] Finished {audio_name} in {duration:.2f} sec ({len(pred_segments)} segments)\")\n",
    "\n",
    "        # Save predictions to JSON\n",
    "        out_file = f\"{output_dir}/{audio_name}_pyannote.json\"\n",
    "        with open(out_file, \"w\") as f:\n",
    "            json.dump(pred_segments, f, indent=2)\n",
    "\n",
    "        # Append summary to results\n",
    "        results.append({\n",
    "            \"audio\": audio_path,\n",
    "            \"n_segments\": len(pred_segments),\n",
    "            \"runtime_sec\": duration,\n",
    "            \"output_file\": out_file\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed on {audio_name}: {e}\")\n",
    "        results.append({\n",
    "            \"audio\": audio_path,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Save overall results as CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"../results/pyannote_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n[INFO] All files processed. Summary saved to ../results/pyannote_summary.csv\")\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c2692",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Metrics\n",
    "This section evaluates diarization results using DER, boundary metrics, and speaker assignment accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000f60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.n.h/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved to ../results/pyannote_eval.csv\n",
      "\n",
      "Per-file (first 8):\n",
      "                                               audio       DER  Boundary_F1  \\\n",
      "0  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...  0.011285     0.181818   \n",
      "1  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...  0.254558     0.368421   \n",
      "2  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...  0.074083     0.375000   \n",
      "3  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...  0.145053     0.490909   \n",
      "4  ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...  0.222181     0.202532   \n",
      "5  ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...  0.094761     0.348485   \n",
      "6  ../audios-wav/12-audios-ar-en/6-audios-en/1_sp...  0.026526     0.533333   \n",
      "7  ../audios-wav/12-audios-ar-en/6-audios-en/1_sp...  0.049459     0.712329   \n",
      "\n",
      "   Speaker_assign_acc  Runtime_sec  N_ref  N_pred  \n",
      "0            0.999973   242.921457      4       7  \n",
      "1            0.753860   219.935136     14      62  \n",
      "2            0.920906   215.254184     13      37  \n",
      "3            0.897477   282.313628     33      80  \n",
      "4            0.775854   362.471811     18     107  \n",
      "5            0.888517   296.227627     25      42  \n",
      "6            0.992190   205.731769     19      11  \n",
      "7            0.940589   202.758552     30      43  \n",
      "\n",
      "Aggregate means:\n",
      "DER                     0.109630\n",
      "Boundary_F1             0.456965\n",
      "Speaker_assign_acc      0.902280\n",
      "Runtime_sec           275.741620\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, pathlib, numpy as np, pandas as pd\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# === Load GT ===\n",
    "df = pd.read_csv(\"../data/refined_dataset.csv\")\n",
    "df[\"segments\"] = df[\"speaker\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "# === Load summary of predictions ===\n",
    "summary = pd.read_csv(\"../results/pyannote_summary.csv\")\n",
    "\n",
    "# Join GT with predictions\n",
    "eval_df = summary.merge(df[[\"audio\", \"segments\", \"speaker_count\"]], on=\"audio\", how=\"left\")\n",
    "\n",
    "# === Helpers ===\n",
    "def segments_to_annotation(segments):\n",
    "    ann = Annotation()\n",
    "    for seg in segments:\n",
    "        start = float(seg[\"start\"]); end = float(seg[\"end\"])\n",
    "        if end > start:\n",
    "            ann[Segment(start, end)] = str(seg[\"labels\"][0])\n",
    "    return ann\n",
    "\n",
    "def load_pred_segments(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_boundaries(segments):\n",
    "    return sorted(set([float(s[\"start\"]) for s in segments] + [float(s[\"end\"]) for s in segments]))\n",
    "\n",
    "def match_boundaries(pred_b, ref_b, tol=0.5):\n",
    "    ref_used = [False]*len(ref_b); matches=[]; TP=0\n",
    "    for p in pred_b:\n",
    "        best=None; best_abs=None; best_idx=None\n",
    "        for i,r in enumerate(ref_b):\n",
    "            if ref_used[i]: continue\n",
    "            d=p-r\n",
    "            if abs(d)<=tol and (best_abs is None or abs(d)<best_abs):\n",
    "                best=(p,r,d); best_abs=abs(d); best_idx=i\n",
    "        if best is not None:\n",
    "            matches.append(best); ref_used[best_idx]=True; TP+=1\n",
    "    FP=len(pred_b)-TP; FN=len(ref_b)-TP\n",
    "    return matches,TP,FP,FN\n",
    "\n",
    "def speaker_assignment_accuracy(ref_ann, hyp_ann):\n",
    "    \"\"\"Permutation-invariant assignment via Hungarian on overlap duration.\"\"\"\n",
    "    total_ref = ref_ann.get_timeline().duration()\n",
    "    if total_ref <= 1e-9:\n",
    "        return np.nan\n",
    "\n",
    "    ref_labels = sorted(set(ref_ann.labels()))\n",
    "    hyp_labels = sorted(set(hyp_ann.labels()))\n",
    "    if not ref_labels or not hyp_labels:\n",
    "        return 0.0\n",
    "\n",
    "    M = np.zeros((len(ref_labels), len(hyp_labels)), dtype=float)\n",
    "\n",
    "    # NOTE: itertracks(yield_label=True) -> (segment, track, label)\n",
    "    for ref_seg, _, r_lab in ref_ann.itertracks(yield_label=True):\n",
    "        for hyp_seg, _, h_lab in hyp_ann.itertracks(yield_label=True):\n",
    "            inter = min(ref_seg.end, hyp_seg.end) - max(ref_seg.start, hyp_seg.start)\n",
    "            if inter > 1e-9:\n",
    "                i = ref_labels.index(r_lab)\n",
    "                j = hyp_labels.index(h_lab)\n",
    "                M[i, j] += inter\n",
    "\n",
    "    if M.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    r_ind, h_ind = linear_sum_assignment(-M)  # maximize overlap\n",
    "    matched_overlap = M[r_ind, h_ind].sum()\n",
    "    return float(matched_overlap / total_ref)\n",
    "\n",
    "# === Evaluation ===\n",
    "der_metric = DiarizationErrorRate(collar=0.5, skip_overlap=False)\n",
    "\n",
    "records=[]\n",
    "for _, row in eval_df.iterrows():\n",
    "    audio = row[\"audio\"]\n",
    "    out_file = row.get(\"output_file\")\n",
    "\n",
    "    if not isinstance(out_file, str) or not pathlib.Path(out_file).exists():\n",
    "        records.append({\"audio\": audio, \"ok\": False, \"error\": \"missing pred\"}); continue\n",
    "\n",
    "    hyp_segments = load_pred_segments(out_file)\n",
    "    ref_segments = row[\"segments\"]\n",
    "\n",
    "    ref_ann = segments_to_annotation(ref_segments)\n",
    "    hyp_ann = segments_to_annotation(hyp_segments)\n",
    "\n",
    "    # DER\n",
    "    der = der_metric(ref_ann, hyp_ann)\n",
    "\n",
    "    # Boundary metrics\n",
    "    ref_b = extract_boundaries(ref_segments)\n",
    "    hyp_b = extract_boundaries(hyp_segments)\n",
    "    matches, TP, FP, FN = match_boundaries(hyp_b, ref_b, tol=0.5)\n",
    "    prec = TP/(TP+FP) if (TP+FP) else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN) else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "    offsets = [m[2] for m in matches]\n",
    "    mad = float(np.median(np.abs(offsets))) if offsets else np.nan\n",
    "\n",
    "    # Speaker assignment accuracy (fixed unpack)\n",
    "    assign_acc = speaker_assignment_accuracy(ref_ann, hyp_ann)\n",
    "\n",
    "    records.append({\n",
    "        \"audio\": audio,\n",
    "        \"ok\": True,\n",
    "        \"DER\": float(der),\n",
    "        \"Boundary_precision\": float(prec),\n",
    "        \"Boundary_recall\": float(rec),\n",
    "        \"Boundary_F1\": float(f1),\n",
    "        \"Boundary_median_abs_offset\": mad,\n",
    "        \"Speaker_assign_acc\": assign_acc,\n",
    "        \"Runtime_sec\": float(row.get(\"runtime_sec\", np.nan)),\n",
    "        \"N_ref\": int(len(ref_segments)),\n",
    "        \"N_pred\": int(len(hyp_segments)),\n",
    "    })\n",
    "\n",
    "py_eval = pd.DataFrame(records)\n",
    "py_eval.to_csv(\"../results/pyannote_eval.csv\", index=False)\n",
    "print(\"[INFO] Saved to ../results/pyannote_eval.csv\")\n",
    "\n",
    "print(\"\\nPer-file (first 8):\")\n",
    "print(py_eval[[\"audio\",\"DER\",\"Boundary_F1\",\"Speaker_assign_acc\",\"Runtime_sec\",\"N_ref\",\"N_pred\"]].head(8))\n",
    "\n",
    "print(\"\\nAggregate means:\")\n",
    "print(py_eval[py_eval[\"ok\"]==True][[\"DER\",\"Boundary_F1\",\"Speaker_assign_acc\",\"Runtime_sec\"]].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ad0ed",
   "metadata": {},
   "source": [
    "## 7. Error Analysis and Aggregate Statistics\n",
    "This section analyzes errors, shows per-file and aggregate statistics, and saves evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68acf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN rows]\n",
      " Empty DataFrame\n",
      "Columns: [audio, DER, Boundary_F1, N_ref, N_pred]\n",
      "Index: []\n",
      "\n",
      "[Most over-segmented]\n",
      "                                                 audio  N_ref  N_pred  \\\n",
      "4   ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...     18     107   \n",
      "1   ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...     14      62   \n",
      "2   ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...     13      37   \n",
      "3   ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...     33      80   \n",
      "11  ../audios-wav/12-audios-ar-en/6-audios-en/3_sp...     30      68   \n",
      "\n",
      "    overseg_ratio  \n",
      "4        5.944444  \n",
      "1        4.428571  \n",
      "2        2.846154  \n",
      "3        2.424242  \n",
      "11       2.266667  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, json, pathlib\n",
    "\n",
    "py_eval = pd.read_csv(\"../results/pyannote_eval.csv\")\n",
    "print(\"[NaN rows]\\n\", py_eval[py_eval.isna().any(axis=1)][[\"audio\",\"DER\",\"Boundary_F1\",\"N_ref\",\"N_pred\"]])\n",
    "\n",
    "# Show top over-segmented files\n",
    "overseg = py_eval.dropna().assign(overseg_ratio=lambda d: d[\"N_pred\"]/d[\"N_ref\"].replace(0,np.nan))\n",
    "print(\"\\n[Most over-segmented]\\n\", overseg.sort_values(\"overseg_ratio\", ascending=False).head(5)[[\"audio\",\"N_ref\",\"N_pred\",\"overseg_ratio\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio-ai)",
   "language": "python",
   "name": "audio-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
