{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22958c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>speaker_count</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../audios-wav/audios-ar/3_speakers_ar/three_sp...</td>\n",
       "      <td>3 Speakers</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../audios-wav/audios-en/2_speakers_en/two_spea...</td>\n",
       "      <td>2 Speakers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../audios-wav/audios-ar/2_speakers_ar/two_spea...</td>\n",
       "      <td>2 Speakers</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../audios-wav/audios-ar/3_speakers_ar/three_sp...</td>\n",
       "      <td>3 Speakers</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../audios-wav/audios-en/2_speakers_en/two_spea...</td>\n",
       "      <td>2 Speakers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio speaker_count language\n",
       "0  ../audios-wav/audios-ar/3_speakers_ar/three_sp...    3 Speakers       ar\n",
       "1  ../audios-wav/audios-en/2_speakers_en/two_spea...    2 Speakers       en\n",
       "2  ../audios-wav/audios-ar/2_speakers_ar/two_spea...    2 Speakers       ar\n",
       "3  ../audios-wav/audios-ar/3_speakers_ar/three_sp...    3 Speakers       ar\n",
       "4  ../audios-wav/audios-en/2_speakers_en/two_spea...    2 Speakers       en"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATASET_CSV = ROOT / \"data\" / \"cleaned_dataset.csv\"\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "print(\"rows:\", len(df)); df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e74e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: three_speakers7_ar.wav | true=3 Speakers | simple_diarizer pred=2 | frames=29519\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pathlib import Path\n",
    "import time, re\n",
    "\n",
    "def to_mono_16k(audio_path: Path):\n",
    "    wav, sr = torchaudio.load(str(audio_path))          # [C,T]\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0)\n",
    "    else:\n",
    "        wav = wav.squeeze(0)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.transforms.Resample(sr, 16000)(wav)\n",
    "    return wav, 16000\n",
    "\n",
    "# MFCC front-end (simple, fast)\n",
    "mfcc_tf = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=20,\n",
    "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40}\n",
    ")\n",
    "\n",
    "def feat_mfcc(wav_16k: torch.Tensor) -> np.ndarray:\n",
    "    with torch.inference_mode():\n",
    "        mfcc = mfcc_tf(wav_16k.unsqueeze(0))  # [1, n_mfcc, frames]\n",
    "        mfcc = mfcc.squeeze(0).transpose(0,1) # [frames, n_mfcc]\n",
    "    return mfcc.cpu().numpy()\n",
    "\n",
    "def choose_k(embs: np.ndarray, ks=(1,2,3)):\n",
    "    best_k, best_score = None, -np.inf\n",
    "    for k in ks:\n",
    "        if k == 1:\n",
    "            score = -1e9\n",
    "        else:\n",
    "            labels = AgglomerativeClustering(n_clusters=k, linkage=\"ward\").fit_predict(embs)\n",
    "            if len(np.unique(labels)) < 2:\n",
    "                score = -1e9\n",
    "            else:\n",
    "                score = float(silhouette_score(embs, labels))\n",
    "        if score > best_score:\n",
    "            best_k, best_score = k, score\n",
    "    if best_k is None:\n",
    "        best_k = 1\n",
    "    return int(best_k)\n",
    "\n",
    "row = df.iloc[0]\n",
    "audio_path = Path(row[\"audio\"]).resolve()\n",
    "wav, sr = to_mono_16k(audio_path)\n",
    "X = feat_mfcc(wav)\n",
    "k = choose_k(X, ks=(1,2,3))\n",
    "print(f\"Testing: {audio_path.name} | true={row['speaker_count']} | simple_diarizer pred={k} | frames={len(X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54a7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] three_speakers7_ar.wav ... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     wav, sr \u001b[38;5;241m=\u001b[39m to_mono_16k(audio_path)\n\u001b[1;32m     21\u001b[0m     X \u001b[38;5;241m=\u001b[39m feat_mfcc(wav)\n\u001b[0;32m---> 22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ pred=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mchoose_k\u001b[0;34m(embs, ks)\u001b[0m\n\u001b[1;32m     34\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mAgglomerativeClustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinkage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(labels)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     38\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py:1114\u001b[0m, in \u001b[0;36mAgglomerativeClustering.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit and return the result of each sample's clustering assignment.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    In addition to fitting, this method also return the result of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m        Cluster labels.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/base.py:719\u001b[0m, in \u001b[0;36mClusterMixin.fit_predict\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    Cluster labels.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py:989\u001b[0m, in \u001b[0;36mAgglomerativeClustering.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the hierarchical clustering from features, or distance matrix.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03m    Returns the fitted instance.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    988\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X, ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py:1061\u001b[0m, in \u001b[0;36mAgglomerativeClustering._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1057\u001b[0m distance_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_threshold\n\u001b[1;32m   1059\u001b[0m return_distance \u001b[38;5;241m=\u001b[39m (distance_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_distances\n\u001b[0;32m-> 1061\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_builder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnectivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnectivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_connected_components_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_leaves_, parents) \u001b[38;5;241m=\u001b[39m out[\n\u001b[1;32m   1069\u001b[0m     :\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m   1070\u001b[0m ]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/joblib/memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py:316\u001b[0m, in \u001b[0;36mward_tree\u001b[0;34m(X, connectivity, n_clusters, return_distance)\u001b[0m\n\u001b[1;32m    304\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    305\u001b[0m         (\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial build of the tree is implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrequire(X, requirements\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m children_ \u001b[38;5;241m=\u001b[39m out[:, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mintp)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/scipy/cluster/hierarchy.py:796\u001b[0m, in \u001b[0;36mward\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mward\u001b[39m(y):\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    Perform Ward's linkage on a condensed distance matrix.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m \n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinkage\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/scipy/cluster/hierarchy.py:1040\u001b[0m, in \u001b[0;36mlinkage\u001b[0;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     result \u001b[38;5;241m=\u001b[39m _hierarchy\u001b[38;5;241m.\u001b[39mmst_single_linkage(y, n)\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mward\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m-> 1040\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_hierarchy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     result \u001b[38;5;241m=\u001b[39m _hierarchy\u001b[38;5;241m.\u001b[39mfast_linkage(y, n, method_code)\n",
      "File \u001b[0;32m_hierarchy.pyx:929\u001b[0m, in \u001b[0;36mscipy.cluster._hierarchy.nn_chain\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/numpy/core/numeric.py:136\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype, order, like)\u001b[0m\n\u001b[1;32m    132\u001b[0m     multiarray\u001b[38;5;241m.\u001b[39mcopyto(res, z, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;129m@set_array_function_like_doc\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mones\u001b[39m(shape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Return a new array of given shape and type, filled with ones.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, time, re\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = (Path.cwd().parents[0] / \"results\") if Path.cwd().name == \"notebooks\" else (Path.cwd() / \"results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV_SIMPLE = RESULTS_DIR / \"simple_predictions.csv\"\n",
    "\n",
    "def true_count(s):\n",
    "    m = re.search(r\"\\d+\", str(s))\n",
    "    return int(m.group()) if m else np.nan\n",
    "\n",
    "rows, failures = [], 0\n",
    "t0_all = time.time()\n",
    "\n",
    "for idx, r in df.iterrows():\n",
    "    audio_path = Path(r[\"audio\"]).resolve()\n",
    "    print(f\"[{idx+1}/{len(df)}] {audio_path.name} ...\", end=\" \", flush=True)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        wav, sr = to_mono_16k(audio_path)\n",
    "        X = feat_mfcc(wav)\n",
    "        pred = choose_k(X, ks=(1,2,3))\n",
    "        status = f\"âœ“ pred={pred}\"\n",
    "    except Exception as e:\n",
    "        pred = np.nan\n",
    "        failures += 1\n",
    "        status = f\"âœ— failed ({e})\"\n",
    "    dt = time.time() - t0\n",
    "    print(f\"{status} | {dt:.1f}s\")\n",
    "\n",
    "    rows.append({\n",
    "        \"audio\": str(audio_path),\n",
    "        \"language\": r[\"language\"],\n",
    "        \"true_speakers\": true_count(r[\"speaker_count\"]),\n",
    "        \"pred_speakers\": pred,\n",
    "        \"runtime_sec\": dt,\n",
    "    })\n",
    "\n",
    "simp_df = pd.DataFrame(rows)\n",
    "simp_df.to_csv(OUT_CSV_SIMPLE, index=False)\n",
    "print(f\"\\nSaved: {OUT_CSV_SIMPLE}\")\n",
    "print(f\"Total rows: {len(simp_df)} | Failures: {failures} | Total runtime: {(time.time()-t0_all)/60:.1f} min\")\n",
    "simp_df.head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca62c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = (Path.cwd().parents[0] / \"results\") if Path.cwd().name == \"notebooks\" else (Path.cwd() / \"results\")\n",
    "OUT_CSV_SIMPLE = RESULTS_DIR / \"simple_predictions.csv\"\n",
    "\n",
    "pred_df = pd.read_csv(OUT_CSV_SIMPLE)\n",
    "\n",
    "y_true = pred_df[\"true_speakers\"].astype(int)\n",
    "y_pred = pred_df[\"pred_speakers\"].fillna(-1).astype(int)\n",
    "\n",
    "print(\"=== SimpleDiarizer Evaluation ===\")\n",
    "print(f\"Accuracy         : {accuracy_score(y_true, y_pred):.2%}\")\n",
    "print(f\"Precision (macro): {precision_score(y_true, y_pred, average='macro', zero_division=0):.2%}\")\n",
    "print(f\"Recall (macro)   : {recall_score(y_true, y_pred, average='macro', zero_division=0):.2%}\")\n",
    "print(f\"F1-score (macro) : {f1_score(y_true, y_pred, average='macro', zero_division=0):.2%}\\n\")\n",
    "print(\"Per-class report:\")\n",
    "print(classification_report(y_true, y_pred, digits=3, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,2,3,-1])\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[1,2,3,\"fail\"], yticklabels=[1,2,3,\"fail\"])\n",
    "plt.title(\"Confusion Matrix - SimpleDiarizer Speaker Count\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d1ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio-ai)",
   "language": "python",
   "name": "audio-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
