{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a518211",
   "metadata": {},
   "source": [
    "# SpeechBrain Diarization Pipeline\n",
    "This notebook performs speaker diarization using the SpeechBrain ECAPA-TDNN model and evaluates the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd26e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: 3.9.23 (main, Jun  3 2025, 18:47:52) \n",
      "[Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "CWD: /Users/s.n.h/Voice-AI/Audio-AI/notebooks\n",
      "GT_CSV exists: True\n",
      "GT rows: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>speaker_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  speaker_count\n",
       "0  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...            1.0\n",
       "1  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...            1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, pathlib, pandas as pd\n",
    "\n",
    "GT_CSV = \"../data/refined_dataset.csv\"\n",
    "SB_PRED_DIR = pathlib.Path(\"../results/speechbrain_predictions\")\n",
    "SB_SUMMARY  = \"../results/speechbrain_summary.csv\"\n",
    "\n",
    "SB_PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PY:\", sys.version)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"GT_CSV exists:\", os.path.exists(GT_CSV))\n",
    "\n",
    "df = pd.read_csv(GT_CSV)\n",
    "print(\"GT rows:\", len(df))\n",
    "df.head(2)[[\"audio\",\"speaker_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a3f71",
   "metadata": {},
   "source": [
    "## 2. Embedding and Clustering Functions\n",
    "This section defines functions for extracting embeddings and clustering for diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd3b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "\n",
    "def windowed_embeddings(wav, sr, win_s=1.0, hop_s=0.5):\n",
    "    W = int(sr*win_s); H = int(sr*hop_s)\n",
    "    embs, times = [], []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(wav)-W+1, H):\n",
    "            chunk = wav[start:start+W].unsqueeze(0).to(torch.device)\n",
    "            emb = sb_enc.encode_batch(chunk).squeeze(0).squeeze(0).cpu().numpy() # type: ignore\n",
    "            embs.append(emb)\n",
    "            times.append((start/sr, (start+W)/sr))\n",
    "    return np.array(embs), times\n",
    "\n",
    "def cluster_auto(embs, k_min=1, k_max=4):\n",
    "    if len(embs) < 2:\n",
    "        return np.zeros(len(embs), dtype=int), 1\n",
    "    best_k, best_score, best_labels = 1, -1.0, np.zeros(len(embs), dtype=int)\n",
    "    for k in range(k_min, min(k_max, len(embs)) + 1):\n",
    "        try:\n",
    "            lab = AgglomerativeClustering(n_clusters=k, linkage=\"ward\").fit_predict(embs)\n",
    "            score = silhouette_score(embs, lab) if k > 1 else -1.0\n",
    "            if score > best_score:\n",
    "                best_k, best_score, best_labels = k, score, lab\n",
    "        except Exception:\n",
    "            pass\n",
    "    return best_labels, best_k\n",
    "\n",
    "def windows_to_segments(times, labels, min_seg=0.30, gap_merge=0.25):\n",
    "    if not times: return []\n",
    "    ordered = sorted(zip(times, labels), key=lambda x: x[0][0])\n",
    "    out = []\n",
    "    cs, ce, cl = ordered[0][0][0], ordered[0][0][1], ordered[0][1]\n",
    "    for (t0, t1), lab in ordered[1:]:\n",
    "        if lab == cl and t0 - ce <= gap_merge:\n",
    "            ce = max(ce, t1)\n",
    "        else:\n",
    "            if ce - cs >= min_seg:\n",
    "                out.append({\"start\": float(cs), \"end\": float(ce), \"labels\": [f\"Speaker {int(cl)+1}\"]})\n",
    "            cs, ce, cl = t0, t1, lab\n",
    "    if ce - cs >= min_seg:\n",
    "        out.append({\"start\": float(cs), \"end\": float(ce), \"labels\": [f\"Speaker {int(cl)+1}\"]})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c7d07",
   "metadata": {},
   "source": [
    "## 3. Audio Processing and VAD Functions\n",
    "This section defines functions for reading audio and performing voice activity detection (VAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2975e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio, torch, os\n",
    "\n",
    "def read_wav(path, target_sr=16000):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Error loading audio file: not found {path}\")\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.dim() > 1:  # make mono\n",
    "        wav = wav.mean(dim=0)\n",
    "    if sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "        sr = target_sr\n",
    "    return wav.squeeze(0), sr\n",
    "\n",
    "# webrtcvad: frames must be exactly 10, 20, or 30 ms; sr must be 8000/16000/32000\n",
    "def frames_vad(wav, sr, frame_ms=30, vad_aggr=2):\n",
    "    assert frame_ms in (10, 20, 30), \"webrtcvad requires 10/20/30 ms frames\"\n",
    "    import webrtcvad\n",
    "    from scipy.signal import medfilt\n",
    "\n",
    "    vad = webrtcvad.Vad(vad_aggr)\n",
    "    frame_len = int(sr * frame_ms / 1000)\n",
    "    hop = frame_len\n",
    "\n",
    "    speech = []\n",
    "    for start in range(0, len(wav), hop):\n",
    "        end = min(start + frame_len, len(wav))\n",
    "        frm = wav[start:end]\n",
    "        if len(frm) < frame_len:\n",
    "            frm = torch.nn.functional.pad(frm, (0, frame_len - len(frm)))\n",
    "        pcm16 = (frm.clamp(-1, 1) * 32767.0).to(torch.int16).cpu().numpy().tobytes()\n",
    "        speech.append(1 if vad.is_speech(pcm16, sr) else 0)\n",
    "\n",
    "    speech = medfilt(torch.tensor(speech, dtype=torch.int32).numpy(), kernel_size=5)\n",
    "\n",
    "    segs = []\n",
    "    i = 0\n",
    "    n = len(speech)\n",
    "    while i < n:\n",
    "        if speech[i] == 1:\n",
    "            j = i + 1\n",
    "            while j < n and speech[j] == 1:\n",
    "                j += 1\n",
    "            segs.append((i, j))\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    segs_sec = []\n",
    "    for s, e in segs:\n",
    "        start = s * frame_len / sr\n",
    "        end = e * frame_len / sr\n",
    "        if end - start >= 0.20:\n",
    "            segs_sec.append((start, end))\n",
    "    return segs_sec, frame_len / sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477f2d2",
   "metadata": {},
   "source": [
    "## 4. SpeechBrain Encoder Setup\n",
    "This section sets up the SpeechBrain ECAPA-TDNN encoder and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "593f7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SpeechBrain device and encoder\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "sb_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sb_enc = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    run_opts={\"device\": str(sb_device)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fa4c0",
   "metadata": {},
   "source": [
    "## 5. Clustering and Segment Conversion Functions\n",
    "This section defines clustering and segment conversion utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07375807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def windowed_embeddings(wav, sr, win_s=1.0, hop_s=0.5):\n",
    "    W = int(sr*win_s); H = int(sr*hop_s)\n",
    "    embs, times = [], []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(wav)-W+1, H):\n",
    "            chunk = wav[start:start+W].unsqueeze(0).to(sb_device)\n",
    "            emb = sb_enc.encode_batch(chunk).squeeze(0).squeeze(0).cpu().numpy()\n",
    "            embs.append(emb)\n",
    "            times.append((start/sr, (start+W)/sr))\n",
    "    return np.array(embs), times\n",
    "\n",
    "def cluster_auto(embs, k_min=1, k_max=4):\n",
    "    if len(embs) < 2:\n",
    "        return np.zeros(len(embs), dtype=int), 1\n",
    "    best_k, best_score, best_labels = 1, -1.0, np.zeros(len(embs), dtype=int)\n",
    "    for k in range(k_min, min(k_max, len(embs)) + 1):\n",
    "        try:\n",
    "            lab = AgglomerativeClustering(n_clusters=k, linkage=\"ward\").fit_predict(embs)\n",
    "            score = silhouette_score(embs, lab) if k > 1 else -1.0\n",
    "            if score > best_score:\n",
    "                best_k, best_score, best_labels = k, score, lab\n",
    "        except Exception:\n",
    "            pass\n",
    "    return best_labels, best_k\n",
    "\n",
    "def windows_to_segments(times, labels, min_seg=0.30, gap_merge=0.25):\n",
    "    if not times: return []\n",
    "    ordered = sorted(zip(times, labels), key=lambda x: x[0][0])\n",
    "    out = []\n",
    "    cs, ce, cl = ordered[0][0][0], ordered[0][0][1], ordered[0][1]\n",
    "    for (t0, t1), lab in ordered[1:]:\n",
    "        if lab == cl and t0 - ce <= gap_merge:\n",
    "            ce = max(ce, t1)\n",
    "        else:\n",
    "            if ce - cs >= min_seg:\n",
    "                out.append({\"start\": float(cs), \"end\": float(ce), \"labels\": [f\"Speaker {int(cl)+1}\"]})\n",
    "            cs, ce, cl = t0, t1, lab\n",
    "    if ce - cs >= min_seg:\n",
    "        out.append({\"start\": float(cs), \"end\": float(ce), \"labels\": [f\"Speaker {int(cl)+1}\"]})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0b245",
   "metadata": {},
   "source": [
    "## 6. Batch Diarization and Result Saving\n",
    "This section runs diarization on all files and saves the results to JSON and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc0194be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] (1/12) solo10_ar\n",
      "  -> 85 segs, 55.03s\n",
      "[SB] (2/12) solo3_ar\n",
      "  -> 85 segs, 55.03s\n",
      "[SB] (2/12) solo3_ar\n",
      "  -> 60 segs, 23.03s\n",
      "[SB] (3/12) two_speakers7_ar\n",
      "  -> 60 segs, 23.03s\n",
      "[SB] (3/12) two_speakers7_ar\n",
      "  -> 36 segs, 41.53s\n",
      "[SB] (4/12) two_speakers10_ar\n",
      "  -> 36 segs, 41.53s\n",
      "[SB] (4/12) two_speakers10_ar\n",
      "  -> 61 segs, 52.15s\n",
      "[SB] (5/12) three_speakers5_ar\n",
      "  -> 61 segs, 52.15s\n",
      "[SB] (5/12) three_speakers5_ar\n",
      "  -> 65 segs, 59.55s\n",
      "[SB] (6/12) three_speakers1_ar\n",
      "  -> 65 segs, 59.55s\n",
      "[SB] (6/12) three_speakers1_ar\n",
      "  -> 42 segs, 63.40s\n",
      "[SB] (7/12) solo3_en\n",
      "  -> 42 segs, 63.40s\n",
      "[SB] (7/12) solo3_en\n",
      "  -> 59 segs, 38.72s\n",
      "[SB] (8/12) solo2_en\n",
      "  -> 59 segs, 38.72s\n",
      "[SB] (8/12) solo2_en\n",
      "  -> 80 segs, 38.32s\n",
      "[SB] (9/12) two_speakers8_en\n",
      "  -> 80 segs, 38.32s\n",
      "[SB] (9/12) two_speakers8_en\n",
      "  -> 65 segs, 58.24s\n",
      "[SB] (10/12) two_speakers7_en\n",
      "  -> 65 segs, 58.24s\n",
      "[SB] (10/12) two_speakers7_en\n",
      "  -> 63 segs, 48.59s\n",
      "[SB] (11/12) three_speakers2_en\n",
      "  -> 63 segs, 48.59s\n",
      "[SB] (11/12) three_speakers2_en\n",
      "  -> 42 segs, 62.81s\n",
      "[SB] (12/12) three_speakers8_en\n",
      "  -> 42 segs, 62.81s\n",
      "[SB] (12/12) three_speakers8_en\n",
      "  -> 57 segs, 94.10s\n",
      "[SB] Done -> ../results/speechbrain_summary.csv\n",
      "  -> 57 segs, 94.10s\n",
      "[SB] Done -> ../results/speechbrain_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import time, json, pathlib, numpy as np, pandas as pd\n",
    "GT_CSV = \"../data/refined_dataset.csv\"\n",
    "if 'df' not in globals():\n",
    "    df = pd.read_csv(GT_CSV)\n",
    "OUT_DIR = pathlib.Path(\"../results/speechbrain_predictions\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUMMARY_CSV = \"../results/speechbrain_summary.csv\"\n",
    "\n",
    "results = []\n",
    "for i, row in df.iterrows():\n",
    "    audio = row[\"audio\"]; stem = pathlib.Path(audio).stem\n",
    "    print(f\"[SB] ({i+1}/{len(df)}) {stem}\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        wav, sr = read_wav(audio, 16000)\n",
    "        vad_segs, _ = frames_vad(wav, sr, frame_ms=30, vad_aggr=2)\n",
    "\n",
    "        all_embs, all_times = [], []\n",
    "        for (s,e) in vad_segs:\n",
    "            seg = wav[int(s*sr):int(e*sr)]\n",
    "            embs, times = windowed_embeddings(seg, sr, win_s=1.0, hop_s=0.5)\n",
    "            times = [(s+a, s+b) for (a,b) in times]\n",
    "            if len(embs):\n",
    "                all_embs.append(embs); all_times.extend(times)\n",
    "\n",
    "        embs = np.vstack(all_embs) if len(all_embs) else np.zeros((0,192))\n",
    "        if len(embs)==0:\n",
    "            preds = []\n",
    "        else:\n",
    "            labels, k = cluster_auto(embs, k_min=1, k_max=4)\n",
    "            preds = windows_to_segments(all_times, labels, min_seg=0.30, gap_merge=0.25)\n",
    "\n",
    "        out_path = OUT_DIR / f\"{stem}_speechbrain.json\"\n",
    "        with open(out_path, \"w\") as f: json.dump(preds, f, indent=2)\n",
    "\n",
    "        dur = time.time() - t0\n",
    "        results.append({\"audio\": audio, \"n_segments\": len(preds), \"runtime_sec\": dur, \"output_file\": str(out_path)})\n",
    "        print(f\"  -> {len(preds)} segs, {dur:.2f}s\")\n",
    "    except Exception as e:\n",
    "        results.append({\"audio\": audio, \"error\": str(e)})\n",
    "        print(f\"  !! ERROR: {e}\")\n",
    "\n",
    "pd.DataFrame(results).to_csv(SUMMARY_CSV, index=False)\n",
    "print(f\"[SB] Done -> {SUMMARY_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af128",
   "metadata": {},
   "source": [
    "## 7. Merge Ground Truth and Predictions\n",
    "This section merges ground truth data with diarization predictions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2509be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ground truth and summary\n",
    "df = pd.read_csv(\"../data/refined_dataset.csv\")\n",
    "summary = pd.read_csv(\"../results/speechbrain_summary.csv\")\n",
    "\n",
    "# Use 'speaker' as 'segments' for evaluation\n",
    "df[\"segments\"] = df[\"speaker\"]\n",
    "\n",
    "# Merge ground truth with predictions\n",
    "eval_df = summary.merge(df[[\"audio\", \"segments\", \"speaker_count\"]], on=\"audio\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11548b9f",
   "metadata": {},
   "source": [
    "## 8. Evaluation Functions and Metrics\n",
    "This section defines evaluation functions and computes diarization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "118b5443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.n.h/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.n.h/Voice-AI/Audio-AI/.venv/lib/python3.9/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved to ../results/speechbrain_eval.csv\n",
      "\n",
      "Per-file (first 8):\n",
      "                                               audio       DER  Boundary_F1  \\\n",
      "0  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...  0.538149     0.070588   \n",
      "1  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...  0.543405     0.303448   \n",
      "2  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...  0.127357     0.326531   \n",
      "3  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...  0.190055     0.382979   \n",
      "4  ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...  0.238083     0.206061   \n",
      "5  ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...  0.109520     0.477612   \n",
      "6  ../audios-wav/12-audios-ar-en/6-audios-en/1_sp...  0.375217     0.428571   \n",
      "7  ../audios-wav/12-audios-ar-en/6-audios-en/1_sp...  0.514689     0.481132   \n",
      "\n",
      "   Speaker_assign_acc  Runtime_sec  N_ref  N_pred  \n",
      "0            0.659828    55.034382      4      85  \n",
      "1            0.508158    23.028747     14      60  \n",
      "2            0.871193    41.525807     13      36  \n",
      "3            0.842380    52.153260     33      61  \n",
      "4            0.761100    59.545855     18      65  \n",
      "5            0.880427    63.398093     25      42  \n",
      "6            0.677461    38.719464     19      59  \n",
      "7            0.582993    38.316007     30      80  \n",
      "\n",
      "Aggregate means:\n",
      "DER                    0.289385\n",
      "Boundary_F1            0.345201\n",
      "Speaker_assign_acc     0.750636\n",
      "Runtime_sec           52.955408\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import json, pathlib\n",
    "\n",
    "def segments_to_annotation(segments):\n",
    "    ann = Annotation()\n",
    "    for seg in segments:\n",
    "        start = float(seg[\"start\"]); end = float(seg[\"end\"])\n",
    "        if end > start:\n",
    "            ann[Segment(start, end)] = str(seg[\"labels\"][0])\n",
    "    return ann\n",
    "\n",
    "def load_pred_segments(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_boundaries(segments):\n",
    "    return sorted(set([float(s[\"start\"]) for s in segments] + [float(s[\"end\"]) for s in segments]))\n",
    "\n",
    "def match_boundaries(pred_b, ref_b, tol=0.5):\n",
    "    ref_used = [False]*len(ref_b); matches=[]; TP=0\n",
    "    for p in pred_b:\n",
    "        best=None; best_abs=None; best_idx=None\n",
    "        for i,r in enumerate(ref_b):\n",
    "            if ref_used[i]: continue\n",
    "            d=p-r\n",
    "            if abs(d)<=tol and (best_abs is None or abs(d)<best_abs):\n",
    "                best=(p,r,d); best_abs=abs(d); best_idx=i\n",
    "        if best is not None:\n",
    "            matches.append(best); ref_used[best_idx]=True; TP+=1\n",
    "    FP=len(pred_b)-TP; FN=len(ref_b)-TP\n",
    "    return matches,TP,FP,FN\n",
    "\n",
    "def speaker_assignment_accuracy(ref_ann, hyp_ann):\n",
    "    total_ref = ref_ann.get_timeline().duration()\n",
    "    if total_ref <= 1e-9:\n",
    "        return np.nan\n",
    "    ref_labels = sorted(set(ref_ann.labels()))\n",
    "    hyp_labels = sorted(set(hyp_ann.labels()))\n",
    "    if not ref_labels or not hyp_labels:\n",
    "        return 0.0\n",
    "    M = np.zeros((len(ref_labels), len(hyp_labels)), dtype=float)\n",
    "    for ref_seg, _, r_lab in ref_ann.itertracks(yield_label=True):\n",
    "        for hyp_seg, _, h_lab in hyp_ann.itertracks(yield_label=True):\n",
    "            inter = min(ref_seg.end, hyp_seg.end) - max(ref_seg.start, hyp_seg.start)\n",
    "            if inter > 1e-9:\n",
    "                i = ref_labels.index(r_lab)\n",
    "                j = hyp_labels.index(h_lab)\n",
    "                M[i, j] += inter\n",
    "    if M.size == 0:\n",
    "        return 0.0\n",
    "    r_ind, h_ind = linear_sum_assignment(-M)\n",
    "    matched_overlap = M[r_ind, h_ind].sum()\n",
    "    return float(matched_overlap / total_ref)\n",
    "\n",
    "# --- Evaluation ---\n",
    "der_metric = DiarizationErrorRate(collar=0.5, skip_overlap=False)\n",
    "\n",
    "records = []\n",
    "for _, row in eval_df.iterrows():\n",
    "    audio = row[\"audio\"]\n",
    "    out_file = row.get(\"output_file\")\n",
    "    if not isinstance(out_file, str) or not pathlib.Path(out_file).exists():\n",
    "        records.append({\"audio\": audio, \"ok\": False, \"error\": \"missing pred\"}); continue\n",
    "    hyp_segments = load_pred_segments(out_file)\n",
    "    ref_segments = json.loads(row[\"segments\"]) if isinstance(row[\"segments\"], str) else row[\"segments\"]\n",
    "    ref_ann = segments_to_annotation(ref_segments)\n",
    "    hyp_ann = segments_to_annotation(hyp_segments)\n",
    "    # DER\n",
    "    der = der_metric(ref_ann, hyp_ann)\n",
    "    # Boundary metrics\n",
    "    ref_b = extract_boundaries(ref_segments)\n",
    "    hyp_b = extract_boundaries(hyp_segments)\n",
    "    matches, TP, FP, FN = match_boundaries(hyp_b, ref_b, tol=0.5)\n",
    "    prec = TP/(TP+FP) if (TP+FP) else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN) else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "    offsets = [m[2] for m in matches]\n",
    "    mad = float(np.median(np.abs(offsets))) if offsets else np.nan\n",
    "    # Speaker assignment accuracy\n",
    "    assign_acc = speaker_assignment_accuracy(ref_ann, hyp_ann)\n",
    "    records.append({\n",
    "        \"audio\": audio,\n",
    "        \"ok\": True,\n",
    "        \"DER\": float(der),\n",
    "        \"Boundary_precision\": float(prec),\n",
    "        \"Boundary_recall\": float(rec),\n",
    "        \"Boundary_F1\": float(f1),\n",
    "        \"Boundary_median_abs_offset\": mad,\n",
    "        \"Speaker_assign_acc\": assign_acc,\n",
    "        \"Runtime_sec\": float(row.get(\"runtime_sec\", np.nan)),\n",
    "        \"N_ref\": int(len(ref_segments)),\n",
    "        \"N_pred\": int(len(hyp_segments)),\n",
    "    })\n",
    "\n",
    "sb_eval = pd.DataFrame(records)\n",
    "sb_eval.to_csv(\"../results/speechbrain_eval.csv\", index=False)\n",
    "print(\"[INFO] Saved to ../results/speechbrain_eval.csv\")\n",
    "\n",
    "print(\"\\nPer-file (first 8):\")\n",
    "print(sb_eval[[\"audio\",\"DER\",\"Boundary_F1\",\"Speaker_assign_acc\",\"Runtime_sec\",\"N_ref\",\"N_pred\"]].head(8))\n",
    "\n",
    "print(\"\\nAggregate means:\")\n",
    "print(sb_eval[sb_eval[\"ok\"]==True][[\"DER\",\"Boundary_F1\",\"Speaker_assign_acc\",\"Runtime_sec\"]].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b0f981",
   "metadata": {},
   "source": [
    "## 9. Run Evaluation and Save Results\n",
    "This section runs the evaluation, computes metrics, and saves the results to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42eac79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN rows]\n",
      " Empty DataFrame\n",
      "Columns: [audio, DER, Boundary_F1, N_ref, N_pred]\n",
      "Index: []\n",
      "\n",
      "[Most over-segmented]\n",
      "                                                audio  N_ref  N_pred  \\\n",
      "0  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...      4      85   \n",
      "1  ../audios-wav/12-audios-ar-en/6-audios-ar/1_sp...     14      60   \n",
      "4  ../audios-wav/12-audios-ar-en/6-audios-ar/3_sp...     18      65   \n",
      "6  ../audios-wav/12-audios-ar-en/6-audios-en/1_sp...     19      59   \n",
      "2  ../audios-wav/12-audios-ar-en/6-audios-ar/2_sp...     13      36   \n",
      "\n",
      "   overseg_ratio  \n",
      "0      21.250000  \n",
      "1       4.285714  \n",
      "4       3.611111  \n",
      "6       3.105263  \n",
      "2       2.769231  \n",
      "\n",
      "[Aggregate means:]\n",
      "DER                    0.289385\n",
      "Boundary_F1            0.345201\n",
      "Speaker_assign_acc     0.750636\n",
      "Runtime_sec           52.955408\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sb_eval = pd.read_csv(\"../results/speechbrain_eval.csv\")\n",
    "\n",
    "# Show rows with any NaN values (potential errors)\n",
    "print(\"[NaN rows]\\n\", sb_eval[sb_eval.isna().any(axis=1)][[\"audio\", \"DER\", \"Boundary_F1\", \"N_ref\", \"N_pred\"]])\n",
    "\n",
    "# Show top over-segmented files\n",
    "overseg = sb_eval.dropna().assign(overseg_ratio=lambda d: d[\"N_pred\"]/d[\"N_ref\"].replace(0, np.nan))\n",
    "print(\"\\n[Most over-segmented]\\n\", overseg.sort_values(\"overseg_ratio\", ascending=False).head(5)[[\"audio\", \"N_ref\", \"N_pred\", \"overseg_ratio\"]])\n",
    "\n",
    "# Aggregate statistics for all successful files\n",
    "print(\"\\n[Aggregate means:]\")\n",
    "print(sb_eval[sb_eval[\"ok\"]==True][[\"DER\",\"Boundary_F1\",\"Speaker_assign_acc\",\"Runtime_sec\"]].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio-ai)",
   "language": "python",
   "name": "audio-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
